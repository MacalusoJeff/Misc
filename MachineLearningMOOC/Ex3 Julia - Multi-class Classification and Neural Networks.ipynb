{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 3 - Julia:  Multi-class Classification and Neural Networks\n",
    "\n",
    "<img src=\"http://dalab.github.io/dissolve-struct/images/multiclass.png\">\n",
    "\n",
    "## Part I: *Multi-class Classification with Logistic Regression*\n",
    "\n",
    "Recognizing handwritten digits.  Though the source is not explicitly mentioned, it is just like the [MNIST database](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "Each value of $X$ will be a 20x20 grid of values representing the grayscale intensity at that location \"unrolled\" into a 400-dimensional vector.  Here is an example for the first number in our data, $X^{(1)}$:\n",
    "\n",
    "$\\hspace{1cm} X^{(1)} = \\begin{bmatrix}x_{(1, 1)}^{(1)},\\ x_{(1, 2)}^{(1)},\\ \\dots\\ x_{(1, 20)}^{(1)} \\\\\n",
    "                                    x_{(2, 1)}^{(1)},\\ x_{(2, 2)}^{(1)},\\ \\dots\\ x_{(2, 20)}^{(1)} \\\\\n",
    "                                    \\vdots \\\\ \n",
    "                                    x_{(20, 1)}^{(1)},\\ x_{(20, 2)}^{(1)},\\ \\dots\\ x_{(20, 20)}^{(1)} \\\\ \n",
    "                    \\end{bmatrix}\n",
    "\\rightarrow \\begin{bmatrix} x_1^{(1)},\\ x_2^{(1)},\\ \\dots\\ x_{400}^{(1)} \\end{bmatrix}\n",
    "\\rightarrow (x^{(1)})^T$\n",
    "\n",
    "Here is our collection of all of the numbers for $X$:\n",
    "\n",
    "$\\hspace{1cm} X = \\begin{bmatrix} (x^{(1)})^T \\\\ (x^{(2)})^T \\\\ \\vdots \\\\ (x^{(400)})^T \\end{bmatrix}$\n",
    "\n",
    "---\n",
    "\n",
    "Beginning with package imports, data loading, and initial visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-30T20:09:08.988000-06:00",
     "start_time": "2017-12-01T02:09:08.989Z"
    }
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "using MAT  # Loading in .mat files\n",
    "using GLM  # For comparing answers\n",
    "using Optim  # For optimizing thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-30T20:09:09.843000-06:00",
     "start_time": "2017-12-01T02:09:09.677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,401)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in our MATLAB matrix\n",
    "file = matopen(\"ex3/ex3data1.mat\")\n",
    "X = read(file, \"X\") # note that this does NOT introduce a variable ``varname`` into scope\n",
    "y = read(file, \"y\")\n",
    "close(file)\n",
    "\n",
    "# Replacing where 0 is marked as 10 in y\n",
    "y[y .> 9] .= 0.0\n",
    "\n",
    "# Flattening from an array into a vector\n",
    "# y\n",
    "\n",
    "# Adding the intercept term for X\n",
    "X = hcat(ones(size(X, 1)), X)\n",
    "\n",
    "size(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun fact, I came across [this blog post](http://zverovich.net/2016/05/13/giving-up-on-julia.html) when searching for a method to replace 10s with 0s.  Once again, it does a pretty good job of summing up my feelings on Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Logistic Regression\n",
    "\n",
    "We'll be taking a matrix multiplication approach to vectorize both the cost function and the gradient for logistic regression.  My previous notebooks for assignment \\#2 ([Python](http://nbviewer.jupyter.org/github/JeffMacaluso/Misc/blob/master/MachineLearningMOOC/Ex2%20Python%20-%20Logistic%20Regression.ipynb) | [R](http://nbviewer.jupyter.org/github/JeffMacaluso/Misc/blob/master/MachineLearningMOOC/Ex2%20R%20-%20Logistic%20Regression.ipynb) | [Julia](http://nbviewer.jupyter.org/github/JeffMacaluso/Misc/blob/master/MachineLearningMOOC/Ex2%20Julia%20-%20Logistic%20Regression.ipynb)) already use a vectorized approach, but I'll have a little more information describing what is happening below.\n",
    "\n",
    "- **Note:** The assignment calls for creating the functions for unregularized logistic regression and later modifying them for regularization, but I'm just going to write the functions for regularized logistic regression right away since the regularization parameter is generally just an addition at the end of the equations.\n",
    "\n",
    "To illustrate this vectorization process, here is how we vectorize our hypothesis, $h_\\theta(x)$, using matrix multiplication with the axiom that $a^Tb = b^Ta$ if $a$ and $b$ are vectors:\n",
    "\n",
    "Defining $X$ and $\\theta$ as\n",
    "\n",
    "$\\hspace{1cm} X = \\begin{bmatrix} (x^{(1)})^T \\\\ (x^{(2)})^T \\\\ \\vdots \\\\ (x^{(m)})^T \\end{bmatrix} \\hspace{0.5cm}$ and $\\hspace{0.5cm} \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix}$\n",
    "\n",
    "Computing matrix product $X\\theta$:\n",
    "\n",
    "$\\hspace{1cm} X\\theta = \\begin{bmatrix} (x^{(1)})^T\\theta \\\\ (x^{(2)})^T\\theta \\\\ \\vdots \\\\ (x^{(m)})^T\\theta \\end{bmatrix} \\hspace{0.5cm} = \\hspace{0.5cm} \\begin{bmatrix} \\theta^T(x^{(1)}) \\\\ \\theta^T(x^{(2)}) \\\\ \\vdots \\\\ \\theta^T(x^{(m)}) \\end{bmatrix}$\n",
    "\n",
    "Basically, the main difference between the vectorized approach and non-vectorized approach is through linear algebra.  The non-vectorized approach would be element-wise (individually compute the cost function/gradient/etc. for each element individually), whereas the vectorized is through matrices.\n",
    "\n",
    "#### Vectorized Cost Function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized Gradient\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\begin{cases} \n",
    "\\hspace{0.25cm} \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} & \\text{for}\\ j = 0 \\\\\n",
    "\\Big(\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\\Big) + \\frac{\\lambda}{m}\\theta_j & \\text{for}\\ j \\geq 1\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding optimal values for $\\theta$\n",
    "\n",
    "Using **[Julia's optimization fmin_cg]** function instead of the fmin function used in the previous exercise because it is more efficient at dealing with a large number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-vs-all Classification\n",
    "\n",
    "In order to predict the digit for each $X^{(i)}$, we need to create a function to do one-vs-all classification.  This means training 10 models (one for each unique value of $y$) to estimate the class likelihood of each digit, and then assign each value of $X^{(i)}$ a digit.  \n",
    "\n",
    "This function first finds the **optimal $\\theta$ values** for each model and places them into a table:\n",
    "\n",
    "|      | 0    | 1    | ...  | 9    |\n",
    "|------|------|------|------|------|\n",
    "|   0  |-8.014|-3.071| ...  |-5.507|\n",
    "|   1  | 0.000| 0.000| ...  | 0.000|\n",
    "| ...  | ...  | ...  | ...  | ...  |\n",
    "| 401  | 0.000| 0.000| ...  | 0.000|\n",
    "\n",
    "\n",
    "And then calculates the **class probabilities** for each value of $X^{(i)}$ and places them into a table:\n",
    "\n",
    "|      | 0    | 1    | ...  | 9    |\n",
    "|------|------|------|------|------|\n",
    "|   0  | 0.999| 0.001| ...  | 0.001|\n",
    "|   1  | 0.999| 0.001| ...  | 0.001|\n",
    "| ...  | ...  | ...  | ...  | ...  |\n",
    "| 5000 | 0.001| 0.001| ...  | 0.999|\n",
    "\n",
    "\n",
    "From here, we just grab the column that has the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy\n",
    "\n",
    "The exercise states that the training accuracy should be around 94.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on accuracy]**.  The accuracy also fluctuates with both the number of iterations (we're keeping it at **[# iterations]** since it's still relatively quick) and the type of optimization algorithm (we're sticking with cg since it's what the exercise recommends).\n",
    "\n",
    "Comparing against Julia's GLM logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on accuracy]**.  Our accuracy is **[lower/higher/the same]** in this case, but we can raise it with increasing our iterations or by trying other optimization algorithms.  Regardless, the accuracy here is largely meaningless since we're not splitting between training/testing/validation sets.\n",
    "\n",
    "Let's look at what we misclassified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on mis-classifications]**\n",
    "\n",
    "---\n",
    "\n",
    "##  Part II: *Neural Networks*\n",
    "\n",
    "Recognizing the same handwritten digits with neural networks\n",
    "\n",
    "### Model Representation\n",
    "\n",
    "Note: Image taken from [scikit-learn documentation](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/multilayerperceptron_network.png\" style=\"width: 300px\">\n",
    "\n",
    "**Input layer $(X_n)$:** 400 nodes (excluding bias node)\n",
    "\n",
    "**Hidden layer $(a_k$ or $s_l)$ :** 25 nodes (in one layer ($L=1$), excluding bias node)\n",
    "\n",
    "**Output layer $(f(X))$:** 10 nodes (binary for each digit)\n",
    "\n",
    "In feed-forward propagation, each node for each step (after the input layer) is calculated multiplying the previous nodes by their respective weights (provided in this exercise, but will be calculated with backpropagation in the next exercise), summing these products, and then performing an activation function - the sigmoid function in this case.  Here is a mathematical representation:\n",
    "\n",
    "$a = f(\\sum_{i=0}^N w_i x_i)$\n",
    "\n",
    "- Notation:\n",
    "    - **$w_i$:** Weight\n",
    "    - **$x_i$:** Node\n",
    "    - **$f()$:** Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy\n",
    "\n",
    "The exercise states that the training accuracy should be around 97.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
